{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is started to download\n",
      "Data is downloaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "path_enron1_ham = \"enron1/ham/\"\n",
    "path_enron1_spam = \"enron1/spam/\"\n",
    "path_enron6_ham = \"enron6/ham/\"\n",
    "path_enron6_spam = \"enron6/spam/\"\n",
    "\n",
    "def create_dt(all_txt_paths,prefix):\n",
    "    enron_list = [] \n",
    "    for item in all_txt_paths:\n",
    "        enron_list.append((Path(prefix+item).read_text(encoding=\"Latin-1\").upper()).replace(\".\",\" \").replace(\",\",\" \").replace(\":\",\" \").replace(';' ,\" \").replace('\\n' ,\" \"))\n",
    "    return enron_list\n",
    "print(\"Data is started to download\")\n",
    "enron1_ham_list = create_dt(os.listdir(path_enron1_ham),path_enron1_ham)\n",
    "enron1_spam_list = create_dt(os.listdir(path_enron1_spam),path_enron1_spam)\n",
    "enron6_ham_list = create_dt(os.listdir(path_enron6_ham),path_enron6_ham)\n",
    "enron6_spam_list = create_dt(os.listdir(path_enron6_spam),path_enron6_spam)\n",
    "\n",
    "print(\"Data is downloaded\") #Data is taken from enron1 and enron6 data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy\n",
    "\n",
    "enron1_ham_list_train,enron1_ham_list_test =train_test_split(enron1_ham_list,test_size=0.3,random_state=42)\n",
    "enron1_spam_list_train,enron1_spam_list_test =train_test_split(enron1_spam_list,test_size=0.3,random_state=42)\n",
    "\n",
    "enron6_ham_list_train,enron6_ham_list_test =train_test_split(enron6_ham_list,test_size=0.3,random_state = 6)\n",
    "enron6_spam_list_train,enron6_spam_list_test =train_test_split(enron6_spam_list,test_size=0.3,random_state=6)\n",
    "#data is splitted into test and train by 0.3 and 0.7 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_finder_from_string(list1):      \n",
    "    list_set = set(list1.split(\" \")) \n",
    "    unique_list = (list(list_set)) \n",
    "    return unique_list\n",
    "\n",
    "def unique_word_lister(data):\n",
    "    word_list = []\n",
    "    unique_list = []\n",
    "    for item in data:\n",
    "        word_list = word_list + unique_finder_from_string(item)\n",
    "    return list(set(word_list))\n",
    "   \n",
    "unique1_ham_train = unique_word_lister(enron1_ham_list_train)\n",
    "unique1_spam_train = unique_word_lister(enron1_spam_list_train)\n",
    "\n",
    "unique6_ham_train = unique_word_lister(enron6_ham_list_train)\n",
    "unique6_spam_train = unique_word_lister(enron6_spam_list_train)\n",
    "\n",
    "#list of words are prepared for both enron1 and enron6 data sets from training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unique1 = list(set(unique1_ham_train + unique1_spam_train)) #unique word list is prepared for enron1\n",
    "all_unique6 = list(set(unique6_ham_train + unique6_spam_train)) #unique word list is prepared for enron6\n",
    "def unique_list_cleaner(unique_list):\n",
    "    the_unique_list = []\n",
    "    for item in unique_list:\n",
    "        if len(item)==1:\n",
    "            continue\n",
    "        if item == \"ON\" or item == \"IN\" or item == \"OF\" or item == \"AT\" or item == \"THE\" or item == \"AN\":\n",
    "            continue\n",
    "        if len(item) <= 3:\n",
    "            flag = 0\n",
    "            for word in item.split():\n",
    "                if word.isalpha():\n",
    "                    flag=1\n",
    "            if flag == 0:\n",
    "                continue\n",
    "        the_unique_list.append(item)\n",
    "    return the_unique_list\n",
    "all_unique1_eliminated = unique_list_cleaner(all_unique1) #unique word list after elimination for enron1\n",
    "all_unique6_eliminated = unique_list_cleaner(all_unique6) #unique word list after elimination for enron6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "enron1_ham_set = pd.DataFrame(data = enron1_ham_list_train) \n",
    "enron1_ham_set[\"SPAM_1\"] = 0\n",
    "enron1_spam_set = pd.DataFrame(data = enron1_spam_list_train) \n",
    "enron1_spam_set[\"SPAM_1\"] = 1\n",
    "enron1_train_set = enron1_ham_set.append(enron1_spam_set) #enron1 train set is prepared with column indicates whether mail is spam or not\n",
    "\n",
    "enron6_ham_set = pd.DataFrame(data = enron6_ham_list_train) \n",
    "enron6_ham_set[\"SPAM_1\"] = 0\n",
    "enron6_spam_set = pd.DataFrame(data = enron6_spam_list_train) \n",
    "enron6_spam_set[\"SPAM_1\"] = 1\n",
    "enron6_train_set = enron6_ham_set.append(enron6_spam_set) #enron6 train set is prepared with column indicates whether mail is spam or not\n",
    "\n",
    "def model_data_creator(data_set,ham_count,spam_count,unique_words): #function which prepares model data with unique word counts\n",
    "    new_columns = unique_words.copy()\n",
    "    count = 2\n",
    "    for item in new_columns:\n",
    "        values = []\n",
    "        ham = 0 \n",
    "        spam = 0\n",
    "        for row in zip(data_set[data_set.columns[0]], data_set[data_set.columns[1]]):\n",
    "            try:\n",
    "                if item in row[0]:\n",
    "                    values.append(1)\n",
    "                    if row[1] == 1:\n",
    "                        spam = spam+row[0].count(item)\n",
    "                    else:\n",
    "                        ham = ham +row[0].count(item)\n",
    "                else:\n",
    "                    values.append(0)\n",
    "            except:\n",
    "                values.append(0)\n",
    "        ham_count.append(ham)\n",
    "        spam_count.append(spam)\n",
    "        data_set.insert(count,item,values,True)\n",
    "        count=count+1\n",
    "    print(\"Function completed\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function completed\n"
     ]
    }
   ],
   "source": [
    "data_set = enron1_train_set.copy()\n",
    "ham_count= []\n",
    "spam_count = []\n",
    "model_data_creator(data_set,ham_count,spam_count,all_unique1_eliminated) #model data for enron1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function completed\n"
     ]
    }
   ],
   "source": [
    "data_set6 = enron6_train_set.copy()\n",
    "ham_count6= []\n",
    "spam_count6 = []\n",
    "model_data_creator(data_set6,ham_count6,spam_count6,all_unique6_eliminated) #model data for enron6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(data_set,test_data,df): #prediction funciton takes data_set,test_data and df -> df and data_set is used to calculate predictions\n",
    "    sonuc = []\n",
    "    say = 0\n",
    "    ham_divider = 0\n",
    "    spam_divider=0\n",
    "    total = 0\n",
    "    ham_divider = df[\"HAM\"].sum() #total of word counts in HAM column\n",
    "    spam_divider = df[\"SPAM\"].sum() #total of word counts in SPAM column\n",
    "    total = ham_divider+spam_divider\n",
    "    for item in zip(test_data[test_data.columns[0]], test_data[test_data.columns[1]]):\n",
    "        spam_prob = 1\n",
    "        ham_prob = 1\n",
    "        count = 0\n",
    "        say = say+1\n",
    "        for row in zip(df[df.columns[0]], df[df.columns[1]],df[df.columns[2]]):\n",
    "            try:\n",
    "                if row[0] in item[0]:\n",
    "                    if row[1] > 0:\n",
    "                        ham_prob = ham_prob * (row[1]/ham_divider)/((row[1]+row[2])/total) #probaility calculation for ham\n",
    "                    else:\n",
    "                        ham_prob = ham_prob*1/ham_divider/((row[1]+row[2])/total) #probaility calculation for ham when zero count occurs\n",
    "                    if row[2] > 0:\n",
    "                        spam_prob = spam_prob * row[2]/spam_divider/((row[1]+row[2])/total)#probaility calculation for spam\n",
    "                    else:\n",
    "                        spam_prob = spam_prob*1/spam_divider/((row[1]+row[2])/total)#probaility calculation for spam when zero count occurs\n",
    "                    ham_prob = ham_prob/(ham_prob+spam_prob) #ham prob normalizer\n",
    "                    spam_prob = spam_prob/(ham_prob+spam_prob) #spam prob normalizer -> normalization occurs at every iteration to be able to avoid zero probability problem for both of spam and ham probabilities\n",
    "                count = count+1\n",
    "            except:\n",
    "                count = count+1\n",
    "                continue\n",
    "        ham_prob = ham_prob*ham_divider/total #probaility is multiplied with ham proability\n",
    "        spam_prob = spam_prob*spam_divider/total #probaility is multiplied with spam proability\n",
    "        if spam_prob == 0 and ham_prob == 0:\n",
    "            print(item[0])\n",
    "            break #works when double zero probability occurs -> probabilities can not be compared in this situation\n",
    "        elif ham_prob>=spam_prob:\n",
    "            sonuc.append(0) # 0 for ham -> if ham_prob > spam_prob then it means ham_prob > 0.5 after normalization\n",
    "        elif spam_prob>ham_prob:\n",
    "            sonuc.append(1) # 1 for spam -> if spam_prob > ham_prob then it means spam_prob > 0.5 after normalization\n",
    "        elif ham_prob == ham_prob*ham_divider/total:\n",
    "            if spam_prob == spam_prob*spam_divider/total:\n",
    "                if ham_prob*ham_divider/total > spam_prob*spam_divider/total:\n",
    "                    sonuc.append(0)\n",
    "                else:\n",
    "                    sonuc.append(1)\n",
    "        elif spam_prob == spam_prob*spam_divider/total:\n",
    "            if ham_prob < 1:\n",
    "                sonuc.append(0)\n",
    "        elif spam_prob == ham_prob:\n",
    "            if ham_prob*ham_divider/total > spam_prob*spam_divider/total:\n",
    "                    sonuc.append(0)\n",
    "            else:\n",
    "                sonuc.append(1)\n",
    "    return sonuc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unique_words_with_hamspam_11 = pd.DataFrame(data = all_unique1_eliminated) \n",
    "all_unique_words_with_hamspam_11[\"HAM\"] = ham_count\n",
    "all_unique_words_with_hamspam_11[\"SPAM\"] = spam_count #this data is used to calculate ham and spam probailities\n",
    "\n",
    "\n",
    "all_unique_words_with_hamspam_66 = pd.DataFrame(data = all_unique6_eliminated)\n",
    "all_unique_words_with_hamspam_66[\"HAM\"] = ham_count6\n",
    "all_unique_words_with_hamspam_66[\"SPAM\"] = spam_count6 #this data is used to calculate ham and spam probailities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "enron1_ham_set_test = pd.DataFrame(data = enron1_ham_list_test) \n",
    "enron1_ham_set_test[\"SPAM_1\"] = 0\n",
    "enron1_spam_set_test = pd.DataFrame(data = enron1_spam_list_test) \n",
    "enron1_spam_set_test[\"SPAM_1\"] = 1\n",
    "enron1_test_set = enron1_ham_set_test.append(enron1_spam_set_test)\n",
    "result_1_1 = pred(data_set,enron1_test_set,all_unique_words_with_hamspam_11) # result for enron1 test set with enron1 train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "enron6_ham_set_test = pd.DataFrame(data = enron6_ham_list_test) \n",
    "enron6_ham_set_test[\"SPAM_1\"] = 0\n",
    "enron6_spam_set_test = pd.DataFrame(data = enron6_spam_list_test) \n",
    "enron6_spam_set_test[\"SPAM_1\"] = 1\n",
    "enron6_test_set = enron1_ham_set_test.append(enron6_spam_set_test)\n",
    "result_6_6 = pred(data_set6,enron6_test_set,all_unique_words_with_hamspam_66)# result for enron6 test set with enron6 train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for enron1 test set with model trained by enron1 train set: \n",
      "TP:  418 TN:  1056 FP:  32 FN:  46 Precision:  0.9497422680412371\n",
      "result for enron6 test set with model trained by enron6 train set: \n",
      "TP:  1277 TN:  1069 FP:  73 FN:  33 Precision:  0.9567699836867863\n"
     ]
    }
   ],
   "source": [
    "def confusion(test_list,pred_list):\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    real = test_list\n",
    "    for i in range(0,len(pred_list)):\n",
    "        if real[i] == 1:\n",
    "            if pred_list[i] == real[i]:\n",
    "                TP = TP+1\n",
    "            else:\n",
    "                FP=FP+1\n",
    "        if real[i] == 0:\n",
    "            if pred_list[i] == real[i]:\n",
    "                TN = TN+1\n",
    "            else:\n",
    "                FN=FN+1\n",
    "\n",
    "    print(\"TP: \",TP,\"TN: \",TN,\"FP: \",FP,\"FN: \",FN,\"Precision: \",(TP+TN)/len(pred_list))\n",
    "print(\"result for enron1 test set with model trained by enron1 train set: \",) #precision stands for accuracy\n",
    "confusion(list(enron1_test_set[\"SPAM_1\"]),result_1_1)\n",
    "print(\"result for enron6 test set with model trained by enron6 train set: \",) #precision stands for accuracy\n",
    "confusion(list(enron6_test_set[\"SPAM_1\"]),result_6_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_6_1 = pred(data_set,enron6_test_set,all_unique_words_with_hamspam_11) # result for enron6 test set with enron1 train set\n",
    "result_1_6 = pred(data_set6,enron1_test_set,all_unique_words_with_hamspam_66)# result for enron1 test set with enron6 train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for enron6 test set with model trained by enron1 train set: \n",
      "TP:  1305 TN:  1056 FP:  45 FN:  46 Precision:  0.9628874388254486\n",
      "result for enron1 test set with model trained by enron6 train set: \n",
      "TP:  420 TN:  1069 FP:  30 FN:  33 Precision:  0.9594072164948454\n"
     ]
    }
   ],
   "source": [
    "print(\"result for enron6 test set with model trained by enron1 train set: \",)\n",
    "confusion(list(enron6_test_set[\"SPAM_1\"]),result_6_1) #precision stands for accuracy\n",
    "print(\"result for enron1 test set with model trained by enron6 train set: \",)\n",
    "confusion(list(enron1_test_set[\"SPAM_1\"]),result_1_6) #precision stands for accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def rule1(data,week,mean,std):\n",
    "    point = data[week]\n",
    "    if point > mean:\n",
    "        if point > mean +3*std:\n",
    "            return True\n",
    "    if point < mean:\n",
    "        if point < mean - 3*std:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "#Functions are returning True if condition is vviolated\n",
    "#Week starts from 1\n",
    "def rule2(data,week,mean):\n",
    "    if(week < 9):\n",
    "        return False\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    last9_points = data[week-9:week]\n",
    "    for item in last9_points:\n",
    "        if item > mean:\n",
    "            count1 = count1 +1\n",
    "        if item < mean:\n",
    "            count2 = count2+1\n",
    "    if count1 == 9 or count2 == 9:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def rule3(data,week):\n",
    "    if week < 6:\n",
    "        return False\n",
    "    last6_points = list(data[week-6:week])\n",
    "    count = 1\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    for i in range (0,len(last6_points)):\n",
    "        if last6_points[i] - last6_points[count]<0:\n",
    "            count1 = count1 + 1\n",
    "        if last6_points[i] - last6_points[count]>0:\n",
    "            count2=count2+1\n",
    "        count = count+1\n",
    "        if count == 6:\n",
    "            break\n",
    "    if count1 == 5 or count2 == 5:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def rule4(data, week):\n",
    "    if week < 14:\n",
    "        return False\n",
    "    last14_weeks = list(data[week-14:week])\n",
    "    flag = 0\n",
    "    if last14_weeks[0] -  last14_weeks[1] > 0  :\n",
    "        flag = 1 #azalış\n",
    "    if  last14_weeks[0] -  last14_weeks[1] < 0 :\n",
    "        flag = 2 #artış\n",
    "    if flag == 0:\n",
    "        return False\n",
    "    count = 2\n",
    "    for i in range (1,len(last14_weeks)): \n",
    "        if last14_weeks[i] - last14_weeks[count]<0:\n",
    "            if flag == 1:\n",
    "                flag = 2\n",
    "            else:\n",
    "                return False\n",
    "        if last14_weeks[i] - last14_weeks[count]>0:\n",
    "            if flag == 2:\n",
    "                flag = 1\n",
    "            else:\n",
    "                return False\n",
    "    return True\n",
    " \n",
    "def rule5(data,week,mean,std):    \n",
    "    if week < 3:\n",
    "        return False\n",
    "    last3_weeks = data[week-3:week]\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    for item in last3_weeks:\n",
    "        if item - 2*std > mean:\n",
    "            count1 = count1 + 1\n",
    "        if item + 2*std < mean:\n",
    "            count2 = count2 + 1\n",
    "    if count1 >=2 or count2 >= 2:\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "def rule6(data,week,mean,std):\n",
    "    if week < 5:\n",
    "        return False\n",
    "    last5_weeks = data[week-5:week]\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    for item in last5_weeks:\n",
    "        if item -std > mean:\n",
    "            count1 = count1 + 1\n",
    "        if item + std < mean:\n",
    "            count2 = count2 + 1\n",
    "    if count1 >=4 or count2 >= 4:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def rule7(data,week,mean,std):\n",
    "    if week < 15:\n",
    "        return False\n",
    "    last15_weeks = data[week-15:week]\n",
    "    upper = std+mean\n",
    "    lower = mean-std\n",
    "\n",
    "    count1 = 0\n",
    "    for item in last15_weeks:\n",
    "        if item > lower and item < upper :\n",
    "            count1 = count1+1\n",
    "    if count1 == 15:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def rule8(data,week,mean,std):\n",
    "    if week < 8:\n",
    "        return False\n",
    "    last8_weeks = list(data[week-8:week])\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    for i in range(0,len(last8_weeks)):\n",
    "        if last8_weeks[i] - std > mean:\n",
    "            count1 = count1 + 1\n",
    "        if last8_weeks[i] + std < mean:\n",
    "            count2 = count2 + 1\n",
    "    if count1 >= 0 and count2 >= 0 and count1+count2 == 8:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "data = pd.read_excel(\"HW1_Question2_dataset.xlsx\")\n",
    "data_train = data[0:364]\n",
    "data_test = data[364:]\n",
    "\n",
    "mean = data_train[\"amount\"].mean()\n",
    "std =  data_train[\"amount\"].std()\n",
    "\n",
    "data_use = data.groupby([\"week\"]).mean()\n",
    "data_amount = data_use[\"amount\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule1:  0 Rule2:  24 Rule3:  16 Rule4:  0 Rule5:  2 Rule6:  14 Rule7:  23 Rule8:  0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "total = []\n",
    "for i in range (1,len(data_amount)+1):\n",
    "    sonuc = []\n",
    "    sonuc.append(i)\n",
    "    if rule1(data_amount, i,mean,std ) and i >= 52:\n",
    "        sonuc.append(1)\n",
    "    else:\n",
    "        sonuc.append(0)\n",
    "    if rule2(data_amount, i,mean ) and i >= 52:\n",
    "        sonuc.append(1)\n",
    "    else:\n",
    "        sonuc.append(0)\n",
    "    if rule3(data_amount, i ) and i >= 52:\n",
    "        sonuc.append(1)\n",
    "    else:\n",
    "        sonuc.append(0)\n",
    "    if rule4(data_amount, i ) and i >= 52:\n",
    "        sonuc.append(1)\n",
    "    else:\n",
    "        sonuc.append(0)\n",
    "    if rule5(data_amount, i,mean,std ) and i >=52:\n",
    "        sonuc.append(1)\n",
    "    else:\n",
    "        sonuc.append(0)\n",
    "    if rule6(data_amount, i,mean,std ) and i >= 52:\n",
    "        sonuc.append(1)\n",
    "    else:\n",
    "        sonuc.append(0)\n",
    "    if rule7(data_amount, i,mean,std ) and i >= 52:\n",
    "        sonuc.append(1)\n",
    "    else:\n",
    "        sonuc.append(0)\n",
    "    if rule8(data_amount, i,mean,std ) and i >= 52:\n",
    "        sonuc.append(1)\n",
    "    else:\n",
    "        sonuc.append(0)\n",
    "    if(i>= 52):\n",
    "        total.append(sonuc)\n",
    "data_frame = pd.DataFrame(data=total)       \n",
    "data_frame.set_axis([\"Week\",\"Rule 1\",\"Rule 2\",\"Rule 3\",\"Rule 4\",\"Rule 5\",\"Rule 6\",\"Rule 7\",\"Rule 8\"], axis = 1 , inplace = True)\n",
    "\n",
    "data_frame.to_csv(\"output.csv\", index=False) #prints out results to csv\n",
    "\n",
    "print(\"Rule1: \",data_frame[\"Rule 1\"].sum(),\"Rule2: \",data_frame[\"Rule 2\"].sum(),\"Rule3: \",data_frame[\"Rule 3\"].sum(),\"Rule4: \",data_frame[\"Rule 4\"].sum(),\n",
    "      \"Rule5: \",data_frame[\"Rule 5\"].sum(),\"Rule6: \",data_frame[\"Rule 6\"].sum(),\"Rule7: \",data_frame[\"Rule 7\"].sum(),\"Rule8: \",data_frame[\"Rule 8\"].sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
